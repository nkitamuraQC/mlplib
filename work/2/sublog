/Users/qclove00/miniforge3/envs/mlplib_env/lib/python3.9/site-packages/fairchem/core/models/scn/spherical_harmonics.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  _Jd = torch.load(os.path.join(os.path.dirname(__file__), "Jd.pt"))
/Users/qclove00/miniforge3/envs/mlplib_env/lib/python3.9/site-packages/fairchem/core/models/escn/so3.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  _Jd = torch.load(os.path.join(os.path.dirname(__file__), "Jd.pt"))
/Users/qclove00/miniforge3/envs/mlplib_env/lib/python3.9/site-packages/fairchem/core/models/equiformer_v2/wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  _Jd = torch.load(os.path.join(os.path.dirname(__file__), "Jd.pt"))
INFO:root:Checking local cache: pretrained_models for model EquiformerV2-31M-S2EF-OC20-All+MD
/Users/qclove00/miniforge3/envs/mlplib_env/lib/python3.9/site-packages/fairchem/core/common/relaxation/ase_utils.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(checkpoint_path, map_location=torch.device("cpu"))
WARNING:root:Detected old config, converting to new format. Consider updating to avoid potential incompatibilities.
INFO:root:amp: true
cmd:
  checkpoint_dir: /Users/qclove00/Desktop/mlplib/work/2/checkpoints/2025-05-31-21-33-04
  commit: core:None,experimental:NA
  identifier: ''
  logs_dir: /Users/qclove00/Desktop/mlplib/work/2/logs/wandb/2025-05-31-21-33-04
  print_every: 100
  results_dir: /Users/qclove00/Desktop/mlplib/work/2/results/2025-05-31-21-33-04
  seed: null
  timestamp_id: 2025-05-31-21-33-04
  version: ''
dataset:
  format: trajectory_lmdb_v2
  grad_target_mean: 0.0
  grad_target_std: 2.887317180633545
  key_mapping:
    force: forces
    y: energy
  normalize_labels: true
  target_mean: -0.7554450631141663
  target_std: 2.887317180633545
  transforms:
    normalizer:
      energy:
        mean: -0.7554450631141663
        stdev: 2.887317180633545
      forces:
        mean: 0.0
        stdev: 2.887317180633545
evaluation_metrics:
  metrics:
    energy:
    - mae
    forces:
    - forcesx_mae
    - forcesy_mae
    - forcesz_mae
    - mae
    - cosine_similarity
    - magnitude_error
    misc:
    - energy_forces_within_threshold
  primary_metric: forces_mae
gp_gpus: null
gpus: 0
logger: wandb
loss_functions:
- energy:
    coefficient: 4
    fn: mae
- forces:
    coefficient: 100
    fn: l2mae
model:
  alpha_drop: 0.1
  attn_activation: silu
  attn_alpha_channels: 64
  attn_hidden_channels: 64
  attn_value_channels: 16
  distance_function: gaussian
  drop_path_rate: 0.1
  edge_channels: 128
  ffn_activation: silu
  ffn_hidden_channels: 128
  grid_resolution: 18
  lmax_list:
  - 4
  max_neighbors: 20
  max_num_elements: 90
  max_radius: 12.0
  mmax_list:
  - 2
  name: equiformer_v2
  norm_type: layer_norm_sh
  num_distance_basis: 512
  num_heads: 8
  num_layers: 8
  num_sphere_samples: 128
  otf_graph: true
  proj_drop: 0.0
  regress_forces: true
  sphere_channels: 128
  use_atom_edge_embedding: true
  use_gate_act: false
  use_grid_mlp: true
  use_pbc: true
  use_s2_act_attn: false
  weight_init: uniform
optim:
  batch_size: 8
  clip_grad_norm: 100
  ema_decay: 0.999
  energy_coefficient: 4
  eval_batch_size: 8
  eval_every: 10000
  force_coefficient: 100
  grad_accumulation_steps: 1
  load_balancing: atoms
  loss_energy: mae
  loss_force: l2mae
  lr_initial: 0.0004
  max_epochs: 3
  num_workers: 8
  optimizer: AdamW
  optimizer_params:
    weight_decay: 0.001
  scheduler: LambdaLR
  scheduler_params:
    epochs: 1009275
    lambda_type: cosine
    lr: 0.0004
    lr_min_factor: 0.01
    warmup_epochs: 3364.25
    warmup_factor: 0.2
outputs:
  energy:
    level: system
  forces:
    eval_on_free_atoms: true
    level: atom
    train_on_free_atoms: true
relax_dataset: {}
slurm:
  additional_parameters:
    constraint: volta32gb
  cpus_per_task: 9
  folder: /checkpoint/abhshkdz/open-catalyst-project/logs/equiformer_v2/8307793
  gpus_per_node: 8
  job_id: '8307793'
  job_name: eq2s_051701_allmd
  mem: 480GB
  nodes: 8
  ntasks_per_node: 8
  partition: learnaccel
  time: 4320
task:
  dataset: trajectory_lmdb_v2
  eval_on_free_atoms: true
  grad_input: atomic forces
  labels:
  - potential energy
  primary_metric: forces_mae
  train_on_free_atoms: true
test_dataset: {}
trainer: ocp
val_dataset: {}

INFO:root:Loading model: equiformer_v2
WARNING:root:equiformer_v2 (EquiformerV2) class is deprecated in favor of equiformer_v2_backbone_and_heads  (EquiformerV2BackboneAndHeads)
INFO:root:Loaded EquiformerV2 with 31058690 parameters.
INFO:root:Loading checkpoint in inference-only mode, not loading keys associated with trainer state!
/Users/qclove00/miniforge3/envs/mlplib_env/lib/python3.9/site-packages/fairchem/core/modules/normalization/normalizer.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  "mean": torch.tensor(state_dict["mean"]),
      Step     Time          Energy          fmax
BFGS:    0 21:32:55       11.470626       33.743290
BFGS:    1 21:32:55       -0.674658        4.562639
BFGS:    2 21:32:55       -0.713096        3.141115
BFGS:    3 21:32:55       -0.746192        1.372810
BFGS:    4 21:32:55       -0.764641        0.665836
BFGS:    5 21:32:55       -0.751621        0.286980
BFGS:    6 21:32:56       -0.740603        0.249755
BFGS:    7 21:32:56       -0.740149        0.308134
BFGS:    8 21:32:56       -0.739784        0.334819
BFGS:    9 21:32:57       -0.737369        0.329409
BFGS:   10 21:32:57       -0.708963        0.245251
BFGS:   11 21:32:57       -0.687973        0.233705
BFGS:   12 21:32:57       -0.691760        0.170688
BFGS:   13 21:32:58       -0.697090        0.152662
BFGS:   14 21:32:58       -0.716077        0.200955
BFGS:   15 21:32:59       -0.732360        0.456784
BFGS:   16 21:32:59       -0.708240        0.751528
BFGS:   17 21:32:59       -0.681496        0.780664
BFGS:   18 21:33:00       -0.562344        0.518890
BFGS:   19 21:33:00       -0.465997        0.394450
BFGS:   20 21:33:01       -0.274056        0.355626
BFGS:   21 21:33:01       -0.318118        0.467059
BFGS:   22 21:33:02       -0.484520        0.433478
BFGS:   23 21:33:02       -0.692556        0.651414
BFGS:   24 21:33:02       -0.681841        0.724292
BFGS:   25 21:33:03       -0.637917        1.017470
BFGS:   26 21:33:03       -0.695728        0.672173
BFGS:   27 21:33:04       -0.722706        0.390650
BFGS:   28 21:33:04       -0.735016        0.151581
BFGS:   29 21:33:05       -0.734010        0.163091
BFGS:   30 21:33:06       -0.733272        0.196983
BFGS:   31 21:33:06       -0.737547        0.244994
BFGS:   32 21:33:07       -0.539242        0.828038
BFGS:   33 21:33:07       -0.598208        0.403461
BFGS:   34 21:33:08       -0.517971        0.393625
BFGS:   35 21:33:09       -0.329891        0.660867
BFGS:   36 21:33:09       -0.424291        0.081776
BFGS:   37 21:33:10       -0.409245        0.106991
BFGS:   38 21:33:10       -0.526671        1.491728
BFGS:   39 21:33:11       -0.453820        0.202816
BFGS:   40 21:33:11       -0.321827        0.766209
BFGS:   41 21:33:12       -0.437337        0.158696
BFGS:   42 21:33:12       -0.429222        0.140893
BFGS:   43 21:33:13       -0.412546        0.123924
BFGS:   44 21:33:14       -0.403462        0.126437
BFGS:   45 21:33:14       -0.340476        0.217322
BFGS:   46 21:33:15       -0.284796        0.451617
BFGS:   47 21:33:16       -0.053949        1.539039
BFGS:   48 21:33:16        0.000893        3.276385
BFGS:   49 21:33:17        0.162967        5.338406
BFGS:   50 21:33:17        0.227827        4.102162
BFGS:   51 21:33:18       -0.415050        2.794790
BFGS:   52 21:33:18       -0.317214        6.443452
BFGS:   53 21:33:19        9.145504       11.964866
BFGS:   54 21:33:20        2.271161        7.699546
BFGS:   55 21:33:20       -0.359528        5.641920
BFGS:   56 21:33:21        0.528004        2.347427
BFGS:   57 21:33:21        0.075551        1.964894
BFGS:   58 21:33:22        0.136542        2.740188
BFGS:   59 21:33:22        0.317773        8.130120
BFGS:   60 21:33:23        3.116132        8.699198
BFGS:   61 21:33:24        0.919464        5.905573
BFGS:   62 21:33:24        2.577322        5.226528
BFGS:   63 21:33:25        5.595002        6.760835
BFGS:   64 21:33:25        7.829296       10.297791
BFGS:   65 21:33:26       11.227675       18.743001
BFGS:   66 21:33:27        9.392739       16.354343
BFGS:   67 21:33:27        8.423101       12.623084
BFGS:   68 21:33:28        7.595553        4.067323
BFGS:   69 21:33:28        8.323241       10.588165
BFGS:   70 21:33:29        8.884379        8.674167
BFGS:   71 21:33:30       10.877140        7.867463
BFGS:   72 21:33:30       12.298858        6.787248
BFGS:   73 21:33:31       14.639001        6.286025
BFGS:   74 21:33:31       14.549969        4.050257
BFGS:   75 21:33:32       15.385035       17.499051
BFGS:   76 21:33:32       16.695801       28.109112
BFGS:   77 21:33:33       16.629732       21.955860
BFGS:   78 21:33:34       16.453690       11.602240
BFGS:   79 21:33:34       16.928289       10.270331
BFGS:   80 21:33:35       15.625584        9.302245
BFGS:   81 21:33:35       14.541617        4.151048
BFGS:   82 21:33:36       15.203007        1.840680
BFGS:   83 21:33:36       16.101076       12.964941
BFGS:   84 21:33:37       15.653412        4.105869
BFGS:   85 21:33:39       14.989522        2.212843
BFGS:   86 21:33:41       15.504045        2.753101
BFGS:   87 21:33:43       15.895586        4.692791
BFGS:   88 21:33:44       15.661148        2.944399
BFGS:   89 21:33:45       15.704817        2.967829
BFGS:   90 21:33:46       15.017857        4.178356
BFGS:   91 21:33:47       16.044268       15.120475
BFGS:   92 21:33:47       17.815653       24.662400
BFGS:   93 21:33:48       17.541748       35.885793
BFGS:   94 21:33:49       18.229805       74.285542
BFGS:   95 21:33:50       19.137125       83.171815
BFGS:   96 21:33:50       20.096869       97.716381
BFGS:   97 21:33:51       18.671005       59.551456
BFGS:   98 21:33:51       17.294201       13.934061
BFGS:   99 21:33:52       16.307827       16.774480
BFGS:  100 21:33:52       15.738646        8.738436
BFGS:  101 21:33:53       15.982525        7.501550
BFGS:  102 21:33:53       16.726143       12.945311
BFGS:  103 21:33:54       18.461946       24.742267
BFGS:  104 21:33:55       17.744221       27.423796
BFGS:  105 21:33:55       18.223047       34.888992
BFGS:  106 21:33:56       19.004234       66.841439
BFGS:  107 21:33:56       19.618143      102.517935
BFGS:  108 21:33:57       20.110527      112.036363
BFGS:  109 21:33:58       20.203653      105.579534
BFGS:  110 21:33:58       20.365000       87.828582
BFGS:  111 21:33:59       19.825470       67.029277
BFGS:  112 21:33:59       17.829573       49.465650
BFGS:  113 21:34:00       15.306440       46.502478
BFGS:  114 21:34:01        8.775404       27.225327
BFGS:  115 21:34:01        7.353769       18.654522
BFGS:  116 21:34:02        4.833933        8.889572
BFGS:  117 21:34:02        3.221288        6.784005
BFGS:  118 21:34:03        1.168113        6.844398
BFGS:  119 21:34:04        0.531993        4.866702
BFGS:  120 21:34:04        0.064194        4.289566
BFGS:  121 21:34:05        0.164897        4.228987
BFGS:  122 21:34:05        0.307713        3.052549
BFGS:  123 21:34:06        0.298627        2.084483
BFGS:  124 21:34:06        0.101217        1.579279
BFGS:  125 21:34:07       -0.156821        1.663168
BFGS:  126 21:34:08       -0.198009        1.074069
BFGS:  127 21:34:08       -0.213302        0.618860
BFGS:  128 21:34:09       -0.200167        0.609383
BFGS:  129 21:34:10       -0.158583        0.708250
BFGS:  130 21:34:12       -0.143476        1.002805
BFGS:  131 21:34:16       -0.114775        1.375638
BFGS:  132 21:34:18       -0.027286        2.201656
BFGS:  133 21:34:19        0.121546        3.301647
BFGS:  134 21:34:19        0.439580        4.733766
BFGS:  135 21:34:20        0.950620        6.293306
BFGS:  136 21:34:21        1.553493        7.864542
BFGS:  137 21:34:21        2.436087        9.558206
BFGS:  138 21:34:22        3.087883       11.968359
BFGS:  139 21:34:22        2.920651       14.120891
BFGS:  140 21:34:22        5.085397       16.095059
BFGS:  141 21:34:23        3.758146       12.808033
BFGS:  142 21:34:23        2.278913        6.623746
BFGS:  143 21:34:24        1.579582        5.243441
BFGS:  144 21:34:24        1.700210        4.731497
BFGS:  145 21:34:25        1.323108        3.596943
BFGS:  146 21:34:25        1.307113        3.466417
BFGS:  147 21:34:25        1.284544        3.412755
BFGS:  148 21:34:26        1.237565        3.072333
BFGS:  149 21:34:26        1.145157        2.392653
BFGS:  150 21:34:27        1.145103        1.742410
BFGS:  151 21:34:27        1.118613        2.148053
BFGS:  152 21:34:28        0.912426        1.995705
BFGS:  153 21:34:28        0.674585        1.920860
BFGS:  154 21:34:29        0.290119        1.753977
BFGS:  155 21:34:29        0.402617        0.645040
BFGS:  156 21:34:30        0.394073        0.545829
BFGS:  157 21:34:30        0.416772        0.571640
BFGS:  158 21:34:31        0.294002        0.514011
BFGS:  159 21:34:31        0.276525        0.481853
BFGS:  160 21:34:32        0.102968        0.728525
BFGS:  161 21:34:32        0.128706        0.575647
BFGS:  162 21:34:32        0.260744        0.375940
BFGS:  163 21:34:33        0.202873        0.340588
BFGS:  164 21:34:33        0.030892        0.369750
BFGS:  165 21:34:34       -0.041797        0.490370
BFGS:  166 21:34:35       -0.078008        0.604824
BFGS:  167 21:34:35       -0.186457        0.538466
BFGS:  168 21:34:36       -0.226598        0.681556
BFGS:  169 21:34:36       -0.231472        0.697877
BFGS:  170 21:34:37       -0.241435        0.640800
BFGS:  171 21:34:37       -0.345831        0.590088
BFGS:  172 21:34:37       -0.455898        0.626999
BFGS:  173 21:34:38       -0.486912        0.652543
BFGS:  174 21:34:38       -0.551252        0.663050
BFGS:  175 21:34:39       -0.616845        0.642053
BFGS:  176 21:34:39       -0.633078        0.692281
BFGS:  177 21:34:40       -0.546673        0.889463
BFGS:  178 21:34:40       -0.263794        1.149479
BFGS:  179 21:34:41        0.166054        1.433193
BFGS:  180 21:34:42        0.378078        1.770487
BFGS:  181 21:34:42        0.615257        1.919949
BFGS:  182 21:34:44        1.098987        2.094127
BFGS:  183 21:34:45        1.494606        2.077555
BFGS:  184 21:34:47        1.750967        1.863126
BFGS:  185 21:34:48        2.014971        1.473076
BFGS:  186 21:34:50        2.155048        1.166550
BFGS:  187 21:34:50        2.134681        1.236226
BFGS:  188 21:34:51        1.998480        1.900987
BFGS:  189 21:34:51        1.687124        2.349907
BFGS:  190 21:34:52        1.445457        2.375023
BFGS:  191 21:34:52        1.570810        1.865107
BFGS:  192 21:34:53        1.622511        1.692611
BFGS:  193 21:34:53        1.697434        1.471192
BFGS:  194 21:34:54        0.737259        1.836342
BFGS:  195 21:34:54        1.101521        1.372203
BFGS:  196 21:34:54        1.088298        1.198766
BFGS:  197 21:34:55        0.836952        1.367167
BFGS:  198 21:34:55        0.857699        1.292625
BFGS:  199 21:34:55        0.872169        0.845392
BFGS:  200 21:34:56        0.713590        0.521314
BFGS:  201 21:34:57        0.503244        0.773052
BFGS:  202 21:34:57        0.593183        0.785773
BFGS:  203 21:34:58        0.602351        0.582548
BFGS:  204 21:34:58        0.609216        0.642298
BFGS:  205 21:34:58        0.639337        0.498907
BFGS:  206 21:34:59        0.506391        0.673887
BFGS:  207 21:34:59        0.685097        0.313412
BFGS:  208 21:35:00        0.663285        0.290685
BFGS:  209 21:35:00        0.662318        0.335816
BFGS:  210 21:35:00        0.448426        0.759821
BFGS:  211 21:35:01        0.561646        0.695034
BFGS:  212 21:35:01        0.595470        0.425427
BFGS:  213 21:35:01        0.589149        0.426138
BFGS:  214 21:35:01        0.181680        0.387074
BFGS:  215 21:35:02        0.537880        0.399349
BFGS:  216 21:35:02        0.655579        0.454913
BFGS:  217 21:35:02       -0.166307        0.570506
BFGS:  218 21:35:03       -0.107217        0.644399
BFGS:  219 21:35:03       -0.045360        0.685369
BFGS:  220 21:35:03       -0.135511        0.583893
BFGS:  221 21:35:03       -0.040620        0.311033
BFGS:  222 21:35:04        0.014164        0.240762
BFGS:  223 21:35:04        0.069922        0.302326
BFGS:  224 21:35:04        0.092999        0.336151
BFGS:  225 21:35:05        0.117984        0.376307
BFGS:  226 21:35:05        0.278407        0.398998
BFGS:  227 21:35:05        0.494558        0.415009
BFGS:  228 21:35:06        0.647754        0.499357
BFGS:  229 21:35:06        0.886561        0.673197
BFGS:  230 21:35:06        1.063576        0.900401
BFGS:  231 21:35:06        1.186240        1.081757
BFGS:  232 21:35:07        1.277103        1.252891
BFGS:  233 21:35:07        1.379812        1.412527
BFGS:  234 21:35:07        1.505630        1.602510
BFGS:  235 21:35:08        1.492356        1.728475
BFGS:  236 21:35:08        1.451960        1.651997
BFGS:  237 21:35:08        1.384278        1.350980
BFGS:  238 21:35:09        1.310545        0.868973
BFGS:  239 21:35:09        1.290854        0.694803
BFGS:  240 21:35:09        1.310640        0.563190
BFGS:  241 21:35:09        1.333228        0.572230
BFGS:  242 21:35:10        1.396441        0.743012
BFGS:  243 21:35:10        1.508947        0.918049
BFGS:  244 21:35:10        1.542629        1.036626
BFGS:  245 21:35:11        1.497397        1.119085
BFGS:  246 21:35:11        1.391464        1.095380
BFGS:  247 21:35:12        1.178559        0.991786
BFGS:  248 21:35:12        0.920432        0.773560
BFGS:  249 21:35:12        0.590039        0.951822
BFGS:  250 21:35:12        0.595239        1.469772
BFGS:  251 21:35:13        0.257578        1.757514
BFGS:  252 21:35:14        0.031649        1.830018
BFGS:  253 21:35:14       -0.226877        1.560866
BFGS:  254 21:35:15       -0.324488        0.738430
BFGS:  255 21:35:16       -0.352437        0.768100
BFGS:  256 21:35:17       -0.333271        0.764126
BFGS:  257 21:35:19       -0.336539        0.847003
BFGS:  258 21:35:25       -0.136248        1.109675
BFGS:  259 21:35:25        0.105281        1.343382
BFGS:  260 21:35:25        0.330101        1.453293
BFGS:  261 21:35:25        0.556821        1.473342
BFGS:  262 21:35:25        0.817753        1.470576
BFGS:  263 21:35:26        0.946958        1.430332
BFGS:  264 21:35:26        0.985857        1.348073
BFGS:  265 21:35:26        0.873828        1.195411
BFGS:  266 21:35:26        0.806959        1.170889
BFGS:  267 21:35:27        0.667743        1.272660
BFGS:  268 21:35:27        0.639897        1.300211
BFGS:  269 21:35:27        0.501433        1.399409
BFGS:  270 21:35:27        0.443963        1.424400
BFGS:  271 21:35:28        0.355256        1.342996
BFGS:  272 21:35:28        0.311709        1.245764
BFGS:  273 21:35:28        0.341968        1.172954
BFGS:  274 21:35:28        0.475340        1.340015
BFGS:  275 21:35:29        0.509952        0.530545
BFGS:  276 21:35:29        0.703633        0.933213
BFGS:  277 21:35:29        0.458834        1.879197
BFGS:  278 21:35:29       -0.095137        2.967300
BFGS:  279 21:35:29        0.610006        3.285695
BFGS:  280 21:35:30        1.198624        3.049252
BFGS:  281 21:35:30        2.397262        2.729490
BFGS:  282 21:35:30        3.968433        3.442126
BFGS:  283 21:35:30        3.565342        9.042367
BFGS:  284 21:35:30        3.975080       21.353095
BFGS:  285 21:35:30        3.858907       43.778317
BFGS:  286 21:35:31        1.297333        6.334870
BFGS:  287 21:35:31        0.634964        6.598160
BFGS:  288 21:35:31        7.106318       37.312534
BFGS:  289 21:35:31        3.902357        4.593421
BFGS:  290 21:35:31        3.088340        1.927531
BFGS:  291 21:35:31        2.804808        1.234047
BFGS:  292 21:35:31        1.413643        1.512141
BFGS:  293 21:35:32        2.346100        4.942952
BFGS:  294 21:35:32        1.482115        1.975909
BFGS:  295 21:35:32        1.405556        1.836889
BFGS:  296 21:35:32        2.203688       10.261225
BFGS:  297 21:35:32        1.425332        1.813667
BFGS:  298 21:35:32        1.180630        1.657498
BFGS:  299 21:35:32        0.619255        3.545414
BFGS:  300 21:35:33        0.708338        1.133996
BFGS:  301 21:35:33        0.470380        0.708152
BFGS:  302 21:35:33        0.544582        1.524671
BFGS:  303 21:35:33        0.473498        1.815395
BFGS:  304 21:35:33        0.408707        2.317735
BFGS:  305 21:35:33        0.347104        1.877341
BFGS:  306 21:35:33        0.380384        0.443836
BFGS:  307 21:35:33        0.408386        0.180651
BFGS:  308 21:35:34        0.563460        0.157927
BFGS:  309 21:35:34        0.549851        0.272254
BFGS:  310 21:35:34        0.817869        1.132563
BFGS:  311 21:35:34        0.726538        0.327563
BFGS:  312 21:35:34        0.826404        0.343605
BFGS:  313 21:35:34        0.839207        0.268418
BFGS:  314 21:35:34        0.923919        0.337758
BFGS:  315 21:35:34        1.239519        0.493240
BFGS:  316 21:35:35        5.162280        0.901296
BFGS:  317 21:35:35       10.959373        1.910992
BFGS:  318 21:35:35       11.451627        2.514428
BFGS:  319 21:35:35       11.339905        1.890918
BFGS:  320 21:35:35        8.569465        0.286057
BFGS:  321 21:35:35        8.827331        0.223229
BFGS:  322 21:35:35        8.148717        0.400783
BFGS:  323 21:35:36        9.121251        0.150805
BFGS:  324 21:35:36        8.735764        0.062631
BFGS:  325 21:35:36        8.483241        0.078518
BFGS:  326 21:35:36        8.305446        0.074178
BFGS:  327 21:35:36        7.542374        0.375426
BFGS:  328 21:35:36        7.650163        0.058039
BFGS:  329 21:35:36        6.903723        1.005178
BFGS:  330 21:35:36       10.004565        0.969849
BFGS:  331 21:35:37        7.394114        0.407343
BFGS:  332 21:35:37        8.097922        0.065646
BFGS:  333 21:35:37        7.676359        0.054328
BFGS:  334 21:35:37        9.833800        0.530134
BFGS:  335 21:35:37        7.156562        0.041347
