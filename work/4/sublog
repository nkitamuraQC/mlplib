/Users/qclove00/miniforge3/envs/mlplib_env/lib/python3.9/site-packages/fairchem/core/models/scn/spherical_harmonics.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  _Jd = torch.load(os.path.join(os.path.dirname(__file__), "Jd.pt"))
/Users/qclove00/miniforge3/envs/mlplib_env/lib/python3.9/site-packages/fairchem/core/models/escn/so3.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  _Jd = torch.load(os.path.join(os.path.dirname(__file__), "Jd.pt"))
/Users/qclove00/miniforge3/envs/mlplib_env/lib/python3.9/site-packages/fairchem/core/models/equiformer_v2/wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  _Jd = torch.load(os.path.join(os.path.dirname(__file__), "Jd.pt"))
INFO:root:Checking local cache: pretrained_models for model EquiformerV2-31M-S2EF-OC20-All+MD
/Users/qclove00/miniforge3/envs/mlplib_env/lib/python3.9/site-packages/fairchem/core/common/relaxation/ase_utils.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(checkpoint_path, map_location=torch.device("cpu"))
WARNING:root:Detected old config, converting to new format. Consider updating to avoid potential incompatibilities.
INFO:root:amp: true
cmd:
  checkpoint_dir: /Users/qclove00/Desktop/mlplib/work/4/checkpoints/2025-05-31-21-33-04
  commit: core:None,experimental:NA
  identifier: ''
  logs_dir: /Users/qclove00/Desktop/mlplib/work/4/logs/wandb/2025-05-31-21-33-04
  print_every: 100
  results_dir: /Users/qclove00/Desktop/mlplib/work/4/results/2025-05-31-21-33-04
  seed: null
  timestamp_id: 2025-05-31-21-33-04
  version: ''
dataset:
  format: trajectory_lmdb_v2
  grad_target_mean: 0.0
  grad_target_std: 2.887317180633545
  key_mapping:
    force: forces
    y: energy
  normalize_labels: true
  target_mean: -0.7554450631141663
  target_std: 2.887317180633545
  transforms:
    normalizer:
      energy:
        mean: -0.7554450631141663
        stdev: 2.887317180633545
      forces:
        mean: 0.0
        stdev: 2.887317180633545
evaluation_metrics:
  metrics:
    energy:
    - mae
    forces:
    - forcesx_mae
    - forcesy_mae
    - forcesz_mae
    - mae
    - cosine_similarity
    - magnitude_error
    misc:
    - energy_forces_within_threshold
  primary_metric: forces_mae
gp_gpus: null
gpus: 0
logger: wandb
loss_functions:
- energy:
    coefficient: 4
    fn: mae
- forces:
    coefficient: 100
    fn: l2mae
model:
  alpha_drop: 0.1
  attn_activation: silu
  attn_alpha_channels: 64
  attn_hidden_channels: 64
  attn_value_channels: 16
  distance_function: gaussian
  drop_path_rate: 0.1
  edge_channels: 128
  ffn_activation: silu
  ffn_hidden_channels: 128
  grid_resolution: 18
  lmax_list:
  - 4
  max_neighbors: 20
  max_num_elements: 90
  max_radius: 12.0
  mmax_list:
  - 2
  name: equiformer_v2
  norm_type: layer_norm_sh
  num_distance_basis: 512
  num_heads: 8
  num_layers: 8
  num_sphere_samples: 128
  otf_graph: true
  proj_drop: 0.0
  regress_forces: true
  sphere_channels: 128
  use_atom_edge_embedding: true
  use_gate_act: false
  use_grid_mlp: true
  use_pbc: true
  use_s2_act_attn: false
  weight_init: uniform
optim:
  batch_size: 8
  clip_grad_norm: 100
  ema_decay: 0.999
  energy_coefficient: 4
  eval_batch_size: 8
  eval_every: 10000
  force_coefficient: 100
  grad_accumulation_steps: 1
  load_balancing: atoms
  loss_energy: mae
  loss_force: l2mae
  lr_initial: 0.0004
  max_epochs: 3
  num_workers: 8
  optimizer: AdamW
  optimizer_params:
    weight_decay: 0.001
  scheduler: LambdaLR
  scheduler_params:
    epochs: 1009275
    lambda_type: cosine
    lr: 0.0004
    lr_min_factor: 0.01
    warmup_epochs: 3364.25
    warmup_factor: 0.2
outputs:
  energy:
    level: system
  forces:
    eval_on_free_atoms: true
    level: atom
    train_on_free_atoms: true
relax_dataset: {}
slurm:
  additional_parameters:
    constraint: volta32gb
  cpus_per_task: 9
  folder: /checkpoint/abhshkdz/open-catalyst-project/logs/equiformer_v2/8307793
  gpus_per_node: 8
  job_id: '8307793'
  job_name: eq2s_051701_allmd
  mem: 480GB
  nodes: 8
  ntasks_per_node: 8
  partition: learnaccel
  time: 4320
task:
  dataset: trajectory_lmdb_v2
  eval_on_free_atoms: true
  grad_input: atomic forces
  labels:
  - potential energy
  primary_metric: forces_mae
  train_on_free_atoms: true
test_dataset: {}
trainer: ocp
val_dataset: {}

INFO:root:Loading model: equiformer_v2
WARNING:root:equiformer_v2 (EquiformerV2) class is deprecated in favor of equiformer_v2_backbone_and_heads  (EquiformerV2BackboneAndHeads)
INFO:root:Loaded EquiformerV2 with 31058690 parameters.
INFO:root:Loading checkpoint in inference-only mode, not loading keys associated with trainer state!
/Users/qclove00/miniforge3/envs/mlplib_env/lib/python3.9/site-packages/fairchem/core/modules/normalization/normalizer.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  "mean": torch.tensor(state_dict["mean"]),
      Step     Time          Energy          fmax
BFGS:    0 21:33:05       17.574686       31.728859
BFGS:    1 21:33:05       18.317297       45.762078
BFGS:    2 21:33:06       12.419148        6.876563
BFGS:    3 21:33:07        9.728510        2.492803
BFGS:    4 21:33:07        9.491806        0.728050
BFGS:    5 21:33:08        9.885710        2.526826
BFGS:    6 21:33:08        9.919546        4.480976
BFGS:    7 21:33:09       10.245576        9.098916
BFGS:    8 21:33:09       11.878509       18.520819
BFGS:    9 21:33:10       15.929955       30.415626
BFGS:   10 21:33:11       13.486980       26.831640
BFGS:   11 21:33:11        7.275353       14.260175
BFGS:   12 21:33:12        2.594739        4.639201
BFGS:   13 21:33:12        1.879019        2.477857
BFGS:   14 21:33:13        1.150899        1.403112
BFGS:   15 21:33:13        1.061675        1.277809
BFGS:   16 21:33:14        0.662361        1.082663
BFGS:   17 21:33:14        0.706561        1.125455
BFGS:   18 21:33:15        0.691197        1.037893
BFGS:   19 21:33:16        0.582666        0.533581
BFGS:   20 21:33:16        0.553089        0.494996
BFGS:   21 21:33:17        0.450502        0.367497
BFGS:   22 21:33:17        0.274596        0.198949
BFGS:   23 21:33:18        0.241691        0.168553
BFGS:   24 21:33:18        0.671032        0.550325
BFGS:   25 21:33:19        0.252148        0.120958
BFGS:   26 21:33:19        0.247734        0.118724
BFGS:   27 21:33:20        0.243368        0.117161
BFGS:   28 21:33:21        0.241304        0.116813
BFGS:   29 21:33:21        0.248751        0.126109
BFGS:   30 21:33:22        0.388401        0.301567
BFGS:   31 21:33:22        0.525345        0.409366
BFGS:   32 21:33:23        1.260088        0.717683
BFGS:   33 21:33:23        1.686718        1.162907
BFGS:   34 21:33:24        2.124575        2.480440
BFGS:   35 21:33:24        1.538548        0.651140
BFGS:   36 21:33:25        1.546537        0.671387
BFGS:   37 21:33:26        1.619756        2.326893
BFGS:   38 21:33:26        1.245857        1.159914
BFGS:   39 21:33:27        0.987683        0.903652
BFGS:   40 21:33:27        0.836550        0.658119
BFGS:   41 21:33:28        0.797141        0.699409
BFGS:   42 21:33:28        0.761111        0.648230
BFGS:   43 21:33:29        0.841175        0.590150
BFGS:   44 21:33:29        0.794048        1.429461
BFGS:   45 21:33:30        0.778435        2.414010
BFGS:   46 21:33:30        0.840893        3.249066
BFGS:   47 21:33:31        0.543909        2.356721
BFGS:   48 21:33:31        0.480852        1.949833
BFGS:   49 21:33:32        0.450348        0.818401
BFGS:   50 21:33:33        0.423806        0.455590
BFGS:   51 21:33:33        0.439943        0.431054
BFGS:   52 21:33:34        0.447562        0.437725
BFGS:   53 21:33:34        0.412401        0.419277
BFGS:   54 21:33:35        0.453081        0.545932
BFGS:   55 21:33:35        0.505429        1.238362
BFGS:   56 21:33:36        0.585905        1.458100
BFGS:   57 21:33:36        0.570477        1.073636
BFGS:   58 21:33:37        0.712815        3.368781
BFGS:   59 21:33:40        0.573860        0.641877
BFGS:   60 21:33:42        0.529511        0.598761
BFGS:   61 21:33:43        0.511546        0.401587
BFGS:   62 21:33:44        0.507322        0.281935
BFGS:   63 21:33:45        0.529187        0.313490
BFGS:   64 21:33:46        0.524581        0.357770
BFGS:   65 21:33:47        0.391598        0.999000
BFGS:   66 21:33:47        0.193993        1.607149
BFGS:   67 21:33:48       -0.025343        1.697005
BFGS:   68 21:33:48       -0.169035        1.563247
BFGS:   69 21:33:49       -0.210126        1.699798
BFGS:   70 21:33:50       -0.186205        1.908216
BFGS:   71 21:33:50       -0.171616        2.310494
BFGS:   72 21:33:51       -0.192430        2.265618
BFGS:   73 21:33:51       -0.235126        1.765253
BFGS:   74 21:33:52       -0.070518        1.189120
BFGS:   75 21:33:52        0.487996        0.917274
BFGS:   76 21:33:53        0.190069        0.949914
BFGS:   77 21:33:53        0.223624        0.926037
BFGS:   78 21:33:54        0.373159        0.921868
BFGS:   79 21:33:54       -0.005678        1.021874
BFGS:   80 21:33:55       -0.070574        0.933320
BFGS:   81 21:33:55       -0.170443        0.709052
BFGS:   82 21:33:56        0.056543        0.908898
BFGS:   83 21:33:56        0.073442        2.137428
BFGS:   84 21:33:57        0.077780        2.728268
BFGS:   85 21:33:58       -0.340876        2.190306
BFGS:   86 21:33:58       -0.022400        0.705761
BFGS:   87 21:33:59       -0.197057        0.467157
BFGS:   88 21:33:59       -0.297791        0.284298
BFGS:   89 21:34:00       -0.296961        0.214221
BFGS:   90 21:34:00       -0.307998        0.225283
BFGS:   91 21:34:01       -0.326782        0.260724
BFGS:   92 21:34:01       -0.327286        0.262378
BFGS:   93 21:34:02       -0.249906        0.389325
BFGS:   94 21:34:02       -0.263210        0.245723
BFGS:   95 21:34:03       -0.227881        0.241471
BFGS:   96 21:34:04       -0.184859        0.768526
BFGS:   97 21:34:04       -0.205085        1.345797
BFGS:   98 21:34:05       -0.147365        1.884045
BFGS:   99 21:34:05       -0.131541        2.148912
BFGS:  100 21:34:06       -0.007373        2.074645
BFGS:  101 21:34:06        0.059680        1.532367
BFGS:  102 21:34:07        0.049845        1.039683
BFGS:  103 21:34:07       -0.172972        0.835295
BFGS:  104 21:34:08       -0.303046        0.662394
BFGS:  105 21:34:08       -0.455606        0.720668
BFGS:  106 21:34:10       -0.292791        0.691612
BFGS:  107 21:34:10        0.155926        2.741662
BFGS:  108 21:34:13        0.155991        2.240824
BFGS:  109 21:34:16       -0.100542        0.796849
BFGS:  110 21:34:18        0.015790        1.386439
BFGS:  111 21:34:19        0.880522        7.141720
BFGS:  112 21:34:19        0.571228        3.121206
BFGS:  113 21:34:20        0.154058        2.168188
BFGS:  114 21:34:21       -0.155377        1.799961
BFGS:  115 21:34:21       -0.220111        1.073340
BFGS:  116 21:34:21       -0.319988        1.714660
BFGS:  117 21:34:22        0.118282        2.255594
BFGS:  118 21:34:22        0.049242        0.925049
BFGS:  119 21:34:23       -0.012889        0.714439
BFGS:  120 21:34:23       -0.041114        0.334854
BFGS:  121 21:34:24        0.002154        0.412426
BFGS:  122 21:34:24        0.039719        0.442073
BFGS:  123 21:34:25        0.009146        0.464162
BFGS:  124 21:34:25        0.057361        1.155101
BFGS:  125 21:34:25       -0.158681        1.395091
BFGS:  126 21:34:26        0.404191        1.420974
BFGS:  127 21:34:26        0.628588        2.389559
BFGS:  128 21:34:27        0.656692        2.015751
BFGS:  129 21:34:27        0.552425        7.049088
BFGS:  130 21:34:27        5.541887       16.271979
BFGS:  131 21:34:28       16.115374       84.863333
BFGS:  132 21:34:28       10.781356       41.867958
BFGS:  133 21:34:29        9.708345       60.597518
BFGS:  134 21:34:29       16.115253      115.327974
BFGS:  135 21:34:30       20.070803      145.355709
BFGS:  136 21:34:30       21.347017      166.161654
BFGS:  137 21:34:31       20.607609      105.497230
BFGS:  138 21:34:31       19.393240       78.701027
BFGS:  139 21:34:32       19.142990       77.747413
BFGS:  140 21:34:32       18.978045       75.410402
BFGS:  141 21:34:32       17.960819       59.165334
BFGS:  142 21:34:33       17.312990      156.667799
BFGS:  143 21:34:33       16.937830       89.503242
BFGS:  144 21:34:34       16.518925       30.875109
BFGS:  145 21:34:35       18.032978     2128.979820
BFGS:  146 21:34:36       16.701126      302.122379
BFGS:  147 21:34:36       16.446014      259.947124
BFGS:  148 21:34:37       14.030855       37.430235
BFGS:  149 21:34:37       16.939842       93.030716
BFGS:  150 21:34:38       15.964739      117.044764
BFGS:  151 21:34:38       15.217597      129.040340
BFGS:  152 21:34:39       14.536098      145.256347
BFGS:  153 21:34:39       13.390350      129.747258
BFGS:  154 21:34:40       10.991457       62.785019
BFGS:  155 21:34:40        7.842802       14.767843
BFGS:  156 21:34:41        6.666059        6.069404
BFGS:  157 21:34:41        5.657885        2.301223
BFGS:  158 21:34:42        4.825410        1.170407
BFGS:  159 21:34:43        4.344212        0.606218
BFGS:  160 21:34:45        3.785516        0.338059
BFGS:  161 21:34:46        3.739578        0.212336
BFGS:  162 21:34:48        3.686354        0.148835
BFGS:  163 21:34:49        3.687578        0.170274
BFGS:  164 21:34:50        3.811503        1.566900
BFGS:  165 21:34:51        4.162766        1.012428
BFGS:  166 21:34:51        3.847376        0.813700
BFGS:  167 21:34:52        4.096764        8.862621
BFGS:  168 21:34:52        3.599837        5.726704
BFGS:  169 21:34:53        2.906814        7.922524
BFGS:  170 21:34:53        1.905032        8.166893
BFGS:  171 21:34:53        0.759283        7.129549
BFGS:  172 21:34:54        1.466593        4.215023
BFGS:  173 21:34:54        1.580935        5.493158
BFGS:  174 21:34:54        2.042369       13.683972
BFGS:  175 21:34:55        5.842211       17.435990
BFGS:  176 21:34:55        8.086394       22.811569
BFGS:  177 21:34:55       10.734571       24.561112
BFGS:  178 21:34:56       10.601441       23.571404
BFGS:  179 21:34:57       11.400144       26.314448
BFGS:  180 21:34:57       12.131460      100.018484
BFGS:  181 21:34:58       15.187139      189.480889
BFGS:  182 21:34:58       16.628073      144.046389
BFGS:  183 21:34:58       17.170803       60.183845
BFGS:  184 21:34:59       15.905048       49.072212
BFGS:  185 21:34:59       12.509093       56.113801
BFGS:  186 21:35:00       11.908751       59.403588
BFGS:  187 21:35:00       11.954628       62.992092
BFGS:  188 21:35:00       11.704013       52.560783
BFGS:  189 21:35:01       12.255493       26.295267
BFGS:  190 21:35:01       10.935486       19.704475
BFGS:  191 21:35:01       13.111518       42.190390
BFGS:  192 21:35:01       15.812243       89.714770
BFGS:  193 21:35:02       17.041519       46.042313
BFGS:  194 21:35:02       16.036594       91.350052
BFGS:  195 21:35:02       15.758205      107.724600
BFGS:  196 21:35:02       16.592335      189.493696
BFGS:  197 21:35:03       17.791859      187.377675
BFGS:  198 21:35:03       16.684795      124.526224
BFGS:  199 21:35:03       16.104681       23.380237
BFGS:  200 21:35:04       16.118181       27.382119
BFGS:  201 21:35:04       17.415062      102.802870
BFGS:  202 21:35:04       16.801577      130.211723
BFGS:  203 21:35:05       16.047575       18.198866
BFGS:  204 21:35:05       16.125616      157.241090
BFGS:  205 21:35:05       16.985033      178.411341
BFGS:  206 21:35:06       17.488310       35.421960
BFGS:  207 21:35:06       17.553801       26.929158
BFGS:  208 21:35:06       16.706196       30.647048
BFGS:  209 21:35:07       16.763906       27.512269
BFGS:  210 21:35:07       16.024277       18.893205
BFGS:  211 21:35:07       15.833168       37.763687
BFGS:  212 21:35:08       15.730804       34.107459
BFGS:  213 21:35:08       14.955498      110.073271
BFGS:  214 21:35:08       16.221985       71.463915
BFGS:  215 21:35:08       16.728094       48.320328
BFGS:  216 21:35:09       16.907581       25.798181
BFGS:  217 21:35:09       16.757225       39.015205
BFGS:  218 21:35:09       15.723909       89.648283
BFGS:  219 21:35:10       16.768524       59.182295
BFGS:  220 21:35:10       16.875769      436.094413
BFGS:  221 21:35:11       17.114841      224.935999
BFGS:  222 21:35:11       17.464588      114.660345
BFGS:  223 21:35:12       18.426882     2526.558685
