/Users/qclove00/miniforge3/envs/mlplib_env/lib/python3.9/site-packages/fairchem/core/models/scn/spherical_harmonics.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  _Jd = torch.load(os.path.join(os.path.dirname(__file__), "Jd.pt"))
/Users/qclove00/miniforge3/envs/mlplib_env/lib/python3.9/site-packages/fairchem/core/models/escn/so3.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  _Jd = torch.load(os.path.join(os.path.dirname(__file__), "Jd.pt"))
/Users/qclove00/miniforge3/envs/mlplib_env/lib/python3.9/site-packages/fairchem/core/models/equiformer_v2/wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  _Jd = torch.load(os.path.join(os.path.dirname(__file__), "Jd.pt"))
INFO:root:Checking local cache: pretrained_models for model EquiformerV2-31M-S2EF-OC20-All+MD
/Users/qclove00/miniforge3/envs/mlplib_env/lib/python3.9/site-packages/fairchem/core/common/relaxation/ase_utils.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(checkpoint_path, map_location=torch.device("cpu"))
WARNING:root:Detected old config, converting to new format. Consider updating to avoid potential incompatibilities.
INFO:root:amp: true
cmd:
  checkpoint_dir: /Users/qclove00/Desktop/mlplib/work/0/checkpoints/2025-05-31-21-33-04
  commit: core:None,experimental:NA
  identifier: ''
  logs_dir: /Users/qclove00/Desktop/mlplib/work/0/logs/wandb/2025-05-31-21-33-04
  print_every: 100
  results_dir: /Users/qclove00/Desktop/mlplib/work/0/results/2025-05-31-21-33-04
  seed: null
  timestamp_id: 2025-05-31-21-33-04
  version: ''
dataset:
  format: trajectory_lmdb_v2
  grad_target_mean: 0.0
  grad_target_std: 2.887317180633545
  key_mapping:
    force: forces
    y: energy
  normalize_labels: true
  target_mean: -0.7554450631141663
  target_std: 2.887317180633545
  transforms:
    normalizer:
      energy:
        mean: -0.7554450631141663
        stdev: 2.887317180633545
      forces:
        mean: 0.0
        stdev: 2.887317180633545
evaluation_metrics:
  metrics:
    energy:
    - mae
    forces:
    - forcesx_mae
    - forcesy_mae
    - forcesz_mae
    - mae
    - cosine_similarity
    - magnitude_error
    misc:
    - energy_forces_within_threshold
  primary_metric: forces_mae
gp_gpus: null
gpus: 0
logger: wandb
loss_functions:
- energy:
    coefficient: 4
    fn: mae
- forces:
    coefficient: 100
    fn: l2mae
model:
  alpha_drop: 0.1
  attn_activation: silu
  attn_alpha_channels: 64
  attn_hidden_channels: 64
  attn_value_channels: 16
  distance_function: gaussian
  drop_path_rate: 0.1
  edge_channels: 128
  ffn_activation: silu
  ffn_hidden_channels: 128
  grid_resolution: 18
  lmax_list:
  - 4
  max_neighbors: 20
  max_num_elements: 90
  max_radius: 12.0
  mmax_list:
  - 2
  name: equiformer_v2
  norm_type: layer_norm_sh
  num_distance_basis: 512
  num_heads: 8
  num_layers: 8
  num_sphere_samples: 128
  otf_graph: true
  proj_drop: 0.0
  regress_forces: true
  sphere_channels: 128
  use_atom_edge_embedding: true
  use_gate_act: false
  use_grid_mlp: true
  use_pbc: true
  use_s2_act_attn: false
  weight_init: uniform
optim:
  batch_size: 8
  clip_grad_norm: 100
  ema_decay: 0.999
  energy_coefficient: 4
  eval_batch_size: 8
  eval_every: 10000
  force_coefficient: 100
  grad_accumulation_steps: 1
  load_balancing: atoms
  loss_energy: mae
  loss_force: l2mae
  lr_initial: 0.0004
  max_epochs: 3
  num_workers: 8
  optimizer: AdamW
  optimizer_params:
    weight_decay: 0.001
  scheduler: LambdaLR
  scheduler_params:
    epochs: 1009275
    lambda_type: cosine
    lr: 0.0004
    lr_min_factor: 0.01
    warmup_epochs: 3364.25
    warmup_factor: 0.2
outputs:
  energy:
    level: system
  forces:
    eval_on_free_atoms: true
    level: atom
    train_on_free_atoms: true
relax_dataset: {}
slurm:
  additional_parameters:
    constraint: volta32gb
  cpus_per_task: 9
  folder: /checkpoint/abhshkdz/open-catalyst-project/logs/equiformer_v2/8307793
  gpus_per_node: 8
  job_id: '8307793'
  job_name: eq2s_051701_allmd
  mem: 480GB
  nodes: 8
  ntasks_per_node: 8
  partition: learnaccel
  time: 4320
task:
  dataset: trajectory_lmdb_v2
  eval_on_free_atoms: true
  grad_input: atomic forces
  labels:
  - potential energy
  primary_metric: forces_mae
  train_on_free_atoms: true
test_dataset: {}
trainer: ocp
val_dataset: {}

INFO:root:Loading model: equiformer_v2
WARNING:root:equiformer_v2 (EquiformerV2) class is deprecated in favor of equiformer_v2_backbone_and_heads  (EquiformerV2BackboneAndHeads)
INFO:root:Loaded EquiformerV2 with 31058690 parameters.
INFO:root:Loading checkpoint in inference-only mode, not loading keys associated with trainer state!
/Users/qclove00/miniforge3/envs/mlplib_env/lib/python3.9/site-packages/fairchem/core/modules/normalization/normalizer.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  "mean": torch.tensor(state_dict["mean"]),
      Step     Time          Energy          fmax
BFGS:    0 21:32:56        0.349766        0.056106
BFGS:    1 21:32:56        0.352971        0.059995
BFGS:    2 21:32:57        0.403251        0.134546
BFGS:    3 21:32:57        0.282496        0.295970
BFGS:    4 21:32:57        0.351731        0.589189
BFGS:    5 21:32:57        0.205316        1.087156
BFGS:    6 21:32:58       -0.338665        1.490895
BFGS:    7 21:32:58       -0.708089        1.248666
BFGS:    8 21:32:58       -0.243762        0.768568
BFGS:    9 21:32:59       -0.187498        0.715195
BFGS:   10 21:32:59       -0.191953        0.669375
BFGS:   11 21:32:59       -0.218935        1.354024
BFGS:   12 21:33:00       -0.213456        1.381696
BFGS:   13 21:33:00        0.118137        1.454306
BFGS:   14 21:33:01        0.052348        1.784877
BFGS:   15 21:33:01       -0.022876        2.351546
BFGS:   16 21:33:02       -0.042422        2.497555
BFGS:   17 21:33:02        0.140785        2.269780
BFGS:   18 21:33:03        0.155687        1.758704
BFGS:   19 21:33:03        0.120463        1.719023
BFGS:   20 21:33:03        0.500068        1.162276
BFGS:   21 21:33:04        0.593764        1.773471
BFGS:   22 21:33:04        0.211168        2.265264
BFGS:   23 21:33:05       -0.131120        2.167958
BFGS:   24 21:33:06       -0.228339        1.378173
BFGS:   25 21:33:06       -0.083477        0.473108
BFGS:   26 21:33:07       -0.197378        0.407488
BFGS:   27 21:33:07       -0.254436        0.554648
BFGS:   28 21:33:08       -0.185416        0.639536
BFGS:   29 21:33:08       -0.156895        0.638841
BFGS:   30 21:33:09       -0.181753        0.547923
BFGS:   31 21:33:10       -0.164547        0.412595
BFGS:   32 21:33:10       -0.226818        0.255651
BFGS:   33 21:33:11       -0.349331        0.284128
BFGS:   34 21:33:11       -0.310568        0.434346
BFGS:   35 21:33:12       -0.169204        1.612164
BFGS:   36 21:33:12       -0.344814        0.833201
BFGS:   37 21:33:13       -0.390968        0.870113
BFGS:   38 21:33:13       -0.269069        1.067958
BFGS:   39 21:33:14       -0.147785        0.903436
BFGS:   40 21:33:15       -0.154114        0.887174
BFGS:   41 21:33:15       -0.409894        0.915577
BFGS:   42 21:33:16       -0.473808        0.729133
BFGS:   43 21:33:16       -0.491169        0.394416
BFGS:   44 21:33:17       -0.564452        0.222355
BFGS:   45 21:33:18       -0.640895        0.236115
BFGS:   46 21:33:18       -0.685214        0.225022
BFGS:   47 21:33:19       -0.647350        0.152342
BFGS:   48 21:33:19       -0.623981        0.115382
BFGS:   49 21:33:20       -0.677998        0.089341
BFGS:   50 21:33:20       -0.680769        0.067337
BFGS:   51 21:33:21       -0.663524        0.052558
BFGS:   52 21:33:21       -0.664081        0.057379
BFGS:   53 21:33:22       -0.701862        0.061090
BFGS:   54 21:33:22       -0.714806        0.058403
BFGS:   55 21:33:23       -0.558789        0.263838
BFGS:   56 21:33:23       -0.361890        0.764503
BFGS:   57 21:33:24       -0.522168        0.310975
BFGS:   58 21:33:25       -0.524669        0.333500
BFGS:   59 21:33:25       -0.349218        0.609359
BFGS:   60 21:33:26       -0.372230        0.368737
BFGS:   61 21:33:26       -0.373855        0.357082
BFGS:   62 21:33:27        0.083098        0.635844
BFGS:   63 21:33:27        0.289743        1.993155
BFGS:   64 21:33:28        3.144791        7.200995
BFGS:   65 21:33:28        7.907585       22.624950
BFGS:   66 21:33:29        9.591481       33.065978
BFGS:   67 21:33:29        4.799608        6.763840
BFGS:   68 21:33:30        8.288716       23.808490
BFGS:   69 21:33:30       12.963473       48.192947
BFGS:   70 21:33:31       15.141190       38.907852
BFGS:   71 21:33:31       16.195471       23.805733
BFGS:   72 21:33:32       17.927153       43.980643
BFGS:   73 21:33:33       18.520645       90.560561
BFGS:   74 21:33:33       19.057356      101.178080
BFGS:   75 21:33:34       19.826334      172.400693
BFGS:   76 21:33:34       20.414505       94.671691
BFGS:   77 21:33:35       18.440983      153.766009
BFGS:   78 21:33:36       16.920050      241.554794
BFGS:   79 21:33:37       17.115969      120.049353
BFGS:   80 21:33:50       19.738642      233.327254
BFGS:   81 21:33:51       18.333485       87.915171
BFGS:   82 21:33:52       18.787647       45.196333
BFGS:   83 21:33:52       17.848537       29.357080
BFGS:   84 21:33:53       16.729584       26.254423
BFGS:   85 21:33:53       20.100250       57.094308
BFGS:   86 21:33:54       18.095329       50.096170
BFGS:   87 21:33:54       15.608953       42.597867
BFGS:   88 21:33:55       16.659391       26.579395
BFGS:   89 21:33:55       17.387882       44.214365
BFGS:   90 21:33:56       19.829920      133.924830
BFGS:   91 21:33:57       19.925869      149.400109
BFGS:   92 21:33:57       19.839447      154.085029
BFGS:   93 21:33:58       19.168493      139.151174
BFGS:   94 21:33:58       18.289772      136.908584
BFGS:   95 21:33:59       17.460182      129.139087
BFGS:   96 21:33:59       16.559872      102.392694
BFGS:   97 21:34:00       16.573252       95.332562
BFGS:   98 21:34:00       16.105930       44.003106
BFGS:   99 21:34:01       16.557211       17.919252
BFGS:  100 21:34:01       17.420639       36.143103
BFGS:  101 21:34:02       17.218616       43.135474
BFGS:  102 21:34:03       17.439821       42.572708
BFGS:  103 21:34:03       17.350622       42.470965
BFGS:  104 21:34:04       17.654104       61.404683
BFGS:  105 21:34:04       17.050707       44.258396
BFGS:  106 21:34:05       16.897638       47.986735
BFGS:  107 21:34:06       17.979086       39.006386
BFGS:  108 21:34:06       17.158915      129.220698
BFGS:  109 21:34:07       18.263115       74.615930
BFGS:  110 21:34:08       19.100677       69.779443
BFGS:  111 21:34:09       18.655905      107.642644
