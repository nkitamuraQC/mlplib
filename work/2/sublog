/Users/qclove00/miniforge3/envs/mlplib_env/lib/python3.9/site-packages/fairchem/core/models/scn/spherical_harmonics.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  _Jd = torch.load(os.path.join(os.path.dirname(__file__), "Jd.pt"))
/Users/qclove00/miniforge3/envs/mlplib_env/lib/python3.9/site-packages/fairchem/core/models/escn/so3.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  _Jd = torch.load(os.path.join(os.path.dirname(__file__), "Jd.pt"))
/Users/qclove00/miniforge3/envs/mlplib_env/lib/python3.9/site-packages/fairchem/core/models/equiformer_v2/wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  _Jd = torch.load(os.path.join(os.path.dirname(__file__), "Jd.pt"))
INFO:root:Checking local cache: pretrained_models for model EquiformerV2-31M-S2EF-OC20-All+MD
/Users/qclove00/miniforge3/envs/mlplib_env/lib/python3.9/site-packages/fairchem/core/common/relaxation/ase_utils.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(checkpoint_path, map_location=torch.device("cpu"))
WARNING:root:Detected old config, converting to new format. Consider updating to avoid potential incompatibilities.
INFO:root:amp: true
cmd:
  checkpoint_dir: /Users/qclove00/Desktop/mlplib/work/2/checkpoints/2025-05-31-21-33-04
  commit: core:None,experimental:NA
  identifier: ''
  logs_dir: /Users/qclove00/Desktop/mlplib/work/2/logs/wandb/2025-05-31-21-33-04
  print_every: 100
  results_dir: /Users/qclove00/Desktop/mlplib/work/2/results/2025-05-31-21-33-04
  seed: null
  timestamp_id: 2025-05-31-21-33-04
  version: ''
dataset:
  format: trajectory_lmdb_v2
  grad_target_mean: 0.0
  grad_target_std: 2.887317180633545
  key_mapping:
    force: forces
    y: energy
  normalize_labels: true
  target_mean: -0.7554450631141663
  target_std: 2.887317180633545
  transforms:
    normalizer:
      energy:
        mean: -0.7554450631141663
        stdev: 2.887317180633545
      forces:
        mean: 0.0
        stdev: 2.887317180633545
evaluation_metrics:
  metrics:
    energy:
    - mae
    forces:
    - forcesx_mae
    - forcesy_mae
    - forcesz_mae
    - mae
    - cosine_similarity
    - magnitude_error
    misc:
    - energy_forces_within_threshold
  primary_metric: forces_mae
gp_gpus: null
gpus: 0
logger: wandb
loss_functions:
- energy:
    coefficient: 4
    fn: mae
- forces:
    coefficient: 100
    fn: l2mae
model:
  alpha_drop: 0.1
  attn_activation: silu
  attn_alpha_channels: 64
  attn_hidden_channels: 64
  attn_value_channels: 16
  distance_function: gaussian
  drop_path_rate: 0.1
  edge_channels: 128
  ffn_activation: silu
  ffn_hidden_channels: 128
  grid_resolution: 18
  lmax_list:
  - 4
  max_neighbors: 20
  max_num_elements: 90
  max_radius: 12.0
  mmax_list:
  - 2
  name: equiformer_v2
  norm_type: layer_norm_sh
  num_distance_basis: 512
  num_heads: 8
  num_layers: 8
  num_sphere_samples: 128
  otf_graph: true
  proj_drop: 0.0
  regress_forces: true
  sphere_channels: 128
  use_atom_edge_embedding: true
  use_gate_act: false
  use_grid_mlp: true
  use_pbc: true
  use_s2_act_attn: false
  weight_init: uniform
optim:
  batch_size: 8
  clip_grad_norm: 100
  ema_decay: 0.999
  energy_coefficient: 4
  eval_batch_size: 8
  eval_every: 10000
  force_coefficient: 100
  grad_accumulation_steps: 1
  load_balancing: atoms
  loss_energy: mae
  loss_force: l2mae
  lr_initial: 0.0004
  max_epochs: 3
  num_workers: 8
  optimizer: AdamW
  optimizer_params:
    weight_decay: 0.001
  scheduler: LambdaLR
  scheduler_params:
    epochs: 1009275
    lambda_type: cosine
    lr: 0.0004
    lr_min_factor: 0.01
    warmup_epochs: 3364.25
    warmup_factor: 0.2
outputs:
  energy:
    level: system
  forces:
    eval_on_free_atoms: true
    level: atom
    train_on_free_atoms: true
relax_dataset: {}
slurm:
  additional_parameters:
    constraint: volta32gb
  cpus_per_task: 9
  folder: /checkpoint/abhshkdz/open-catalyst-project/logs/equiformer_v2/8307793
  gpus_per_node: 8
  job_id: '8307793'
  job_name: eq2s_051701_allmd
  mem: 480GB
  nodes: 8
  ntasks_per_node: 8
  partition: learnaccel
  time: 4320
task:
  dataset: trajectory_lmdb_v2
  eval_on_free_atoms: true
  grad_input: atomic forces
  labels:
  - potential energy
  primary_metric: forces_mae
  train_on_free_atoms: true
test_dataset: {}
trainer: ocp
val_dataset: {}

INFO:root:Loading model: equiformer_v2
WARNING:root:equiformer_v2 (EquiformerV2) class is deprecated in favor of equiformer_v2_backbone_and_heads  (EquiformerV2BackboneAndHeads)
INFO:root:Loaded EquiformerV2 with 31058690 parameters.
INFO:root:Loading checkpoint in inference-only mode, not loading keys associated with trainer state!
/Users/qclove00/miniforge3/envs/mlplib_env/lib/python3.9/site-packages/fairchem/core/modules/normalization/normalizer.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  "mean": torch.tensor(state_dict["mean"]),
      Step     Time          Energy          fmax
BFGS:    0 21:32:55       11.470626       33.743290
BFGS:    1 21:32:55       -0.674658        4.562639
BFGS:    2 21:32:55       -0.713096        3.141115
BFGS:    3 21:32:55       -0.746192        1.372810
BFGS:    4 21:32:55       -0.764641        0.665836
BFGS:    5 21:32:55       -0.751621        0.286980
BFGS:    6 21:32:56       -0.740603        0.249755
BFGS:    7 21:32:56       -0.740149        0.308134
BFGS:    8 21:32:56       -0.739784        0.334819
BFGS:    9 21:32:57       -0.737369        0.329409
BFGS:   10 21:32:57       -0.708963        0.245251
BFGS:   11 21:32:57       -0.687973        0.233705
BFGS:   12 21:32:57       -0.691760        0.170688
BFGS:   13 21:32:58       -0.697090        0.152662
BFGS:   14 21:32:58       -0.716077        0.200955
BFGS:   15 21:32:59       -0.732360        0.456784
BFGS:   16 21:32:59       -0.708240        0.751528
BFGS:   17 21:32:59       -0.681496        0.780664
BFGS:   18 21:33:00       -0.562344        0.518890
BFGS:   19 21:33:00       -0.465997        0.394450
BFGS:   20 21:33:01       -0.274056        0.355626
BFGS:   21 21:33:01       -0.318118        0.467059
BFGS:   22 21:33:02       -0.484520        0.433478
BFGS:   23 21:33:02       -0.692556        0.651414
BFGS:   24 21:33:02       -0.681841        0.724292
BFGS:   25 21:33:03       -0.637917        1.017470
BFGS:   26 21:33:03       -0.695728        0.672173
BFGS:   27 21:33:04       -0.722706        0.390650
BFGS:   28 21:33:04       -0.735016        0.151581
BFGS:   29 21:33:05       -0.734010        0.163091
BFGS:   30 21:33:06       -0.733272        0.196983
BFGS:   31 21:33:06       -0.737547        0.244994
BFGS:   32 21:33:07       -0.539242        0.828038
BFGS:   33 21:33:07       -0.598208        0.403461
BFGS:   34 21:33:08       -0.517971        0.393625
BFGS:   35 21:33:09       -0.329891        0.660867
BFGS:   36 21:33:09       -0.424291        0.081776
BFGS:   37 21:33:10       -0.409245        0.106991
BFGS:   38 21:33:10       -0.526671        1.491728
BFGS:   39 21:33:11       -0.453820        0.202816
BFGS:   40 21:33:11       -0.321827        0.766209
BFGS:   41 21:33:12       -0.437337        0.158696
BFGS:   42 21:33:12       -0.429222        0.140893
BFGS:   43 21:33:13       -0.412546        0.123924
BFGS:   44 21:33:14       -0.403462        0.126437
BFGS:   45 21:33:14       -0.340476        0.217322
BFGS:   46 21:33:15       -0.284796        0.451617
BFGS:   47 21:33:16       -0.053949        1.539039
BFGS:   48 21:33:16        0.000893        3.276385
BFGS:   49 21:33:17        0.162967        5.338406
BFGS:   50 21:33:17        0.227827        4.102162
BFGS:   51 21:33:18       -0.415050        2.794790
BFGS:   52 21:33:18       -0.317214        6.443452
BFGS:   53 21:33:19        9.145504       11.964866
BFGS:   54 21:33:20        2.271161        7.699546
BFGS:   55 21:33:20       -0.359528        5.641920
BFGS:   56 21:33:21        0.528004        2.347427
BFGS:   57 21:33:21        0.075551        1.964894
BFGS:   58 21:33:22        0.136542        2.740188
BFGS:   59 21:33:22        0.317773        8.130120
BFGS:   60 21:33:23        3.116132        8.699198
BFGS:   61 21:33:24        0.919464        5.905573
BFGS:   62 21:33:24        2.577322        5.226528
BFGS:   63 21:33:25        5.595002        6.760835
BFGS:   64 21:33:25        7.829296       10.297791
BFGS:   65 21:33:26       11.227675       18.743001
BFGS:   66 21:33:27        9.392739       16.354343
BFGS:   67 21:33:27        8.423101       12.623084
BFGS:   68 21:33:28        7.595553        4.067323
BFGS:   69 21:33:28        8.323241       10.588165
BFGS:   70 21:33:29        8.884379        8.674167
BFGS:   71 21:33:30       10.877140        7.867463
BFGS:   72 21:33:30       12.298858        6.787248
BFGS:   73 21:33:31       14.639001        6.286025
BFGS:   74 21:33:31       14.549969        4.050257
BFGS:   75 21:33:32       15.385035       17.499051
BFGS:   76 21:33:32       16.695801       28.109112
BFGS:   77 21:33:33       16.629732       21.955860
BFGS:   78 21:33:34       16.453690       11.602240
BFGS:   79 21:33:34       16.928289       10.270331
BFGS:   80 21:33:35       15.625584        9.302245
BFGS:   81 21:33:35       14.541617        4.151048
BFGS:   82 21:33:36       15.203007        1.840680
BFGS:   83 21:33:36       16.101076       12.964941
BFGS:   84 21:33:37       15.653412        4.105869
BFGS:   85 21:33:39       14.989522        2.212843
BFGS:   86 21:33:41       15.504045        2.753101
BFGS:   87 21:33:43       15.895586        4.692791
BFGS:   88 21:33:44       15.661148        2.944399
BFGS:   89 21:33:45       15.704817        2.967829
BFGS:   90 21:33:46       15.017857        4.178356
BFGS:   91 21:33:47       16.044268       15.120475
BFGS:   92 21:33:47       17.815653       24.662400
BFGS:   93 21:33:48       17.541748       35.885793
BFGS:   94 21:33:49       18.229805       74.285542
BFGS:   95 21:33:50       19.137125       83.171815
BFGS:   96 21:33:50       20.096869       97.716381
BFGS:   97 21:33:51       18.671005       59.551456
BFGS:   98 21:33:51       17.294201       13.934061
BFGS:   99 21:33:52       16.307827       16.774480
BFGS:  100 21:33:52       15.738646        8.738436
BFGS:  101 21:33:53       15.982525        7.501550
BFGS:  102 21:33:53       16.726143       12.945311
BFGS:  103 21:33:54       18.461946       24.742267
BFGS:  104 21:33:55       17.744221       27.423796
BFGS:  105 21:33:55       18.223047       34.888992
BFGS:  106 21:33:56       19.004234       66.841439
BFGS:  107 21:33:56       19.618143      102.517935
BFGS:  108 21:33:57       20.110527      112.036363
BFGS:  109 21:33:58       20.203653      105.579534
BFGS:  110 21:33:58       20.365000       87.828582
BFGS:  111 21:33:59       19.825470       67.029277
BFGS:  112 21:33:59       17.829573       49.465650
BFGS:  113 21:34:00       15.306440       46.502478
BFGS:  114 21:34:01        8.775404       27.225327
BFGS:  115 21:34:01        7.353769       18.654522
BFGS:  116 21:34:02        4.833933        8.889572
BFGS:  117 21:34:02        3.221288        6.784005
BFGS:  118 21:34:03        1.168113        6.844398
BFGS:  119 21:34:04        0.531993        4.866702
BFGS:  120 21:34:04        0.064194        4.289566
BFGS:  121 21:34:05        0.164897        4.228987
BFGS:  122 21:34:05        0.307713        3.052549
BFGS:  123 21:34:06        0.298627        2.084483
BFGS:  124 21:34:06        0.101217        1.579279
BFGS:  125 21:34:07       -0.156821        1.663168
BFGS:  126 21:34:08       -0.198009        1.074069
BFGS:  127 21:34:08       -0.213302        0.618860
BFGS:  128 21:34:09       -0.200167        0.609383
BFGS:  129 21:34:10       -0.158583        0.708250
BFGS:  130 21:34:12       -0.143476        1.002805
BFGS:  131 21:34:16       -0.114775        1.375638
BFGS:  132 21:34:18       -0.027286        2.201656
BFGS:  133 21:34:19        0.121546        3.301647
BFGS:  134 21:34:19        0.439580        4.733766
BFGS:  135 21:34:20        0.950620        6.293306
BFGS:  136 21:34:21        1.553493        7.864542
BFGS:  137 21:34:21        2.436087        9.558206
BFGS:  138 21:34:22        3.087883       11.968359
BFGS:  139 21:34:22        2.920651       14.120891
BFGS:  140 21:34:22        5.085397       16.095059
BFGS:  141 21:34:23        3.758146       12.808033
BFGS:  142 21:34:23        2.278913        6.623746
BFGS:  143 21:34:24        1.579582        5.243441
BFGS:  144 21:34:24        1.700210        4.731497
BFGS:  145 21:34:25        1.323108        3.596943
BFGS:  146 21:34:25        1.307113        3.466417
BFGS:  147 21:34:25        1.284544        3.412755
BFGS:  148 21:34:26        1.237565        3.072333
BFGS:  149 21:34:26        1.145157        2.392653
BFGS:  150 21:34:27        1.145103        1.742410
BFGS:  151 21:34:27        1.118613        2.148053
BFGS:  152 21:34:28        0.912426        1.995705
BFGS:  153 21:34:28        0.674585        1.920860
BFGS:  154 21:34:29        0.290119        1.753977
BFGS:  155 21:34:29        0.402617        0.645040
BFGS:  156 21:34:30        0.394073        0.545829
BFGS:  157 21:34:30        0.416772        0.571640
BFGS:  158 21:34:31        0.294002        0.514011
BFGS:  159 21:34:31        0.276525        0.481853
BFGS:  160 21:34:32        0.102968        0.728525
BFGS:  161 21:34:32        0.128706        0.575647
BFGS:  162 21:34:32        0.260744        0.375940
BFGS:  163 21:34:33        0.202873        0.340588
BFGS:  164 21:34:33        0.030892        0.369750
BFGS:  165 21:34:34       -0.041797        0.490370
BFGS:  166 21:34:35       -0.078008        0.604824
BFGS:  167 21:34:35       -0.186457        0.538466
