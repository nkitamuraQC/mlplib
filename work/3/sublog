/Users/qclove00/miniforge3/envs/mlplib_env/lib/python3.9/site-packages/fairchem/core/models/scn/spherical_harmonics.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  _Jd = torch.load(os.path.join(os.path.dirname(__file__), "Jd.pt"))
/Users/qclove00/miniforge3/envs/mlplib_env/lib/python3.9/site-packages/fairchem/core/models/escn/so3.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  _Jd = torch.load(os.path.join(os.path.dirname(__file__), "Jd.pt"))
/Users/qclove00/miniforge3/envs/mlplib_env/lib/python3.9/site-packages/fairchem/core/models/equiformer_v2/wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  _Jd = torch.load(os.path.join(os.path.dirname(__file__), "Jd.pt"))
INFO:root:Checking local cache: pretrained_models for model EquiformerV2-31M-S2EF-OC20-All+MD
/Users/qclove00/miniforge3/envs/mlplib_env/lib/python3.9/site-packages/fairchem/core/common/relaxation/ase_utils.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(checkpoint_path, map_location=torch.device("cpu"))
WARNING:root:Detected old config, converting to new format. Consider updating to avoid potential incompatibilities.
INFO:root:amp: true
cmd:
  checkpoint_dir: /Users/qclove00/Desktop/mlplib/work/3/checkpoints/2025-05-31-21-33-04
  commit: core:None,experimental:NA
  identifier: ''
  logs_dir: /Users/qclove00/Desktop/mlplib/work/3/logs/wandb/2025-05-31-21-33-04
  print_every: 100
  results_dir: /Users/qclove00/Desktop/mlplib/work/3/results/2025-05-31-21-33-04
  seed: null
  timestamp_id: 2025-05-31-21-33-04
  version: ''
dataset:
  format: trajectory_lmdb_v2
  grad_target_mean: 0.0
  grad_target_std: 2.887317180633545
  key_mapping:
    force: forces
    y: energy
  normalize_labels: true
  target_mean: -0.7554450631141663
  target_std: 2.887317180633545
  transforms:
    normalizer:
      energy:
        mean: -0.7554450631141663
        stdev: 2.887317180633545
      forces:
        mean: 0.0
        stdev: 2.887317180633545
evaluation_metrics:
  metrics:
    energy:
    - mae
    forces:
    - forcesx_mae
    - forcesy_mae
    - forcesz_mae
    - mae
    - cosine_similarity
    - magnitude_error
    misc:
    - energy_forces_within_threshold
  primary_metric: forces_mae
gp_gpus: null
gpus: 0
logger: wandb
loss_functions:
- energy:
    coefficient: 4
    fn: mae
- forces:
    coefficient: 100
    fn: l2mae
model:
  alpha_drop: 0.1
  attn_activation: silu
  attn_alpha_channels: 64
  attn_hidden_channels: 64
  attn_value_channels: 16
  distance_function: gaussian
  drop_path_rate: 0.1
  edge_channels: 128
  ffn_activation: silu
  ffn_hidden_channels: 128
  grid_resolution: 18
  lmax_list:
  - 4
  max_neighbors: 20
  max_num_elements: 90
  max_radius: 12.0
  mmax_list:
  - 2
  name: equiformer_v2
  norm_type: layer_norm_sh
  num_distance_basis: 512
  num_heads: 8
  num_layers: 8
  num_sphere_samples: 128
  otf_graph: true
  proj_drop: 0.0
  regress_forces: true
  sphere_channels: 128
  use_atom_edge_embedding: true
  use_gate_act: false
  use_grid_mlp: true
  use_pbc: true
  use_s2_act_attn: false
  weight_init: uniform
optim:
  batch_size: 8
  clip_grad_norm: 100
  ema_decay: 0.999
  energy_coefficient: 4
  eval_batch_size: 8
  eval_every: 10000
  force_coefficient: 100
  grad_accumulation_steps: 1
  load_balancing: atoms
  loss_energy: mae
  loss_force: l2mae
  lr_initial: 0.0004
  max_epochs: 3
  num_workers: 8
  optimizer: AdamW
  optimizer_params:
    weight_decay: 0.001
  scheduler: LambdaLR
  scheduler_params:
    epochs: 1009275
    lambda_type: cosine
    lr: 0.0004
    lr_min_factor: 0.01
    warmup_epochs: 3364.25
    warmup_factor: 0.2
outputs:
  energy:
    level: system
  forces:
    eval_on_free_atoms: true
    level: atom
    train_on_free_atoms: true
relax_dataset: {}
slurm:
  additional_parameters:
    constraint: volta32gb
  cpus_per_task: 9
  folder: /checkpoint/abhshkdz/open-catalyst-project/logs/equiformer_v2/8307793
  gpus_per_node: 8
  job_id: '8307793'
  job_name: eq2s_051701_allmd
  mem: 480GB
  nodes: 8
  ntasks_per_node: 8
  partition: learnaccel
  time: 4320
task:
  dataset: trajectory_lmdb_v2
  eval_on_free_atoms: true
  grad_input: atomic forces
  labels:
  - potential energy
  primary_metric: forces_mae
  train_on_free_atoms: true
test_dataset: {}
trainer: ocp
val_dataset: {}

INFO:root:Loading model: equiformer_v2
WARNING:root:equiformer_v2 (EquiformerV2) class is deprecated in favor of equiformer_v2_backbone_and_heads  (EquiformerV2BackboneAndHeads)
INFO:root:Loaded EquiformerV2 with 31058690 parameters.
INFO:root:Loading checkpoint in inference-only mode, not loading keys associated with trainer state!
/Users/qclove00/miniforge3/envs/mlplib_env/lib/python3.9/site-packages/fairchem/core/modules/normalization/normalizer.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  "mean": torch.tensor(state_dict["mean"]),
      Step     Time          Energy          fmax
BFGS:    0 21:33:00        4.489593        2.381577
BFGS:    1 21:33:00        4.430151        2.916030
BFGS:    2 21:33:01        5.069595        0.465207
BFGS:    3 21:33:01        4.698445        0.473439
BFGS:    4 21:33:02        4.832502        2.556466
BFGS:    5 21:33:02        4.252335        3.373559
BFGS:    6 21:33:02        7.941700       12.692154
BFGS:    7 21:33:03       16.769909       25.755287
BFGS:    8 21:33:03       16.877363       24.456925
BFGS:    9 21:33:04       17.408785       24.486439
BFGS:   10 21:33:04       17.651178       25.125216
BFGS:   11 21:33:05       17.286121       26.408069
BFGS:   12 21:33:06       16.295410       26.810920
BFGS:   13 21:33:06       13.536565       32.642156
BFGS:   14 21:33:07        4.987265       18.134677
BFGS:   15 21:33:07        1.928956        4.313570
BFGS:   16 21:33:08        1.068006        2.006013
BFGS:   17 21:33:08        0.050910        2.433355
BFGS:   18 21:33:09        0.184948        3.198556
BFGS:   19 21:33:10        2.337830        2.601156
BFGS:   20 21:33:10        2.133636        1.883846
BFGS:   21 21:33:11        2.044800        1.623480
BFGS:   22 21:33:11        1.791480        1.567472
BFGS:   23 21:33:12        0.697938        1.082051
BFGS:   24 21:33:12        0.727336        1.156357
BFGS:   25 21:33:13        0.379612        1.357820
BFGS:   26 21:33:13        0.227669        0.927399
BFGS:   27 21:33:14        0.477900        2.495838
BFGS:   28 21:33:14        0.637556        1.957520
BFGS:   29 21:33:15        0.524209        2.233300
BFGS:   30 21:33:16        0.368435        1.630214
BFGS:   31 21:33:16        0.379976        0.842908
BFGS:   32 21:33:17        0.300551        0.688742
BFGS:   33 21:33:17        0.140060        0.723139
BFGS:   34 21:33:18        0.086499        0.697089
BFGS:   35 21:33:18        0.185422        0.594554
BFGS:   36 21:33:19        0.532834        0.599486
BFGS:   37 21:33:19        0.332714        1.500194
BFGS:   38 21:33:20        0.327396        0.899146
BFGS:   39 21:33:21       -0.118158        0.861687
BFGS:   40 21:33:21       -0.399783        0.508100
BFGS:   41 21:33:22       -0.353868        0.491356
BFGS:   42 21:33:22       -0.435743        0.478984
BFGS:   43 21:33:23       -0.515352        0.511061
BFGS:   44 21:33:23       -0.462473        0.415924
BFGS:   45 21:33:24       -0.402670        0.392120
BFGS:   46 21:33:25       -0.333767        0.436794
BFGS:   47 21:33:25       -0.278320        0.395962
BFGS:   48 21:33:26       -0.232449        0.349920
BFGS:   49 21:33:26       -0.248297        0.285164
BFGS:   50 21:33:27       -0.250016        0.302821
BFGS:   51 21:33:27       -0.233945        0.375505
BFGS:   52 21:33:28       -0.237834        0.607922
BFGS:   53 21:33:28       -0.220394        0.437486
BFGS:   54 21:33:29       -0.340444        0.427687
BFGS:   55 21:33:29       -0.297627        0.395122
BFGS:   56 21:33:30       -0.225124        1.327183
BFGS:   57 21:33:30       -0.183701        0.162848
BFGS:   58 21:33:31       -0.165194        0.175063
BFGS:   59 21:33:31       -0.081989        0.341120
BFGS:   60 21:33:32        0.027888        0.521704
BFGS:   61 21:33:33        0.243422        0.852792
BFGS:   62 21:33:33        0.316014        0.783168
BFGS:   63 21:33:34        0.407631        0.465921
BFGS:   64 21:33:34        0.523363        1.036699
BFGS:   65 21:33:35        0.377259        0.656278
BFGS:   66 21:33:35        0.317847        0.373817
BFGS:   67 21:33:36        0.301091        0.356502
BFGS:   68 21:33:37        0.133845        0.409953
BFGS:   69 21:33:37       -0.126785        0.335965
BFGS:   70 21:33:39       -0.109684        0.206892
BFGS:   71 21:33:41        0.035639        0.142251
BFGS:   72 21:33:43        0.202097        0.348084
BFGS:   73 21:33:44        0.331239        0.718580
BFGS:   74 21:33:45        0.458353        0.998748
BFGS:   75 21:33:46        0.634337        1.138655
BFGS:   76 21:33:47        0.787431        1.054957
BFGS:   77 21:33:47        0.958628        1.017874
BFGS:   78 21:33:48        0.716230        0.829910
BFGS:   79 21:33:48        0.543825        0.926700
BFGS:   80 21:33:49        0.559623        1.007457
BFGS:   81 21:33:50        0.602583        1.143989
BFGS:   82 21:33:50        0.554477        1.271536
BFGS:   83 21:33:51        0.455180        1.146896
BFGS:   84 21:33:51        0.278582        0.939779
BFGS:   85 21:33:52       -0.046725        0.955566
BFGS:   86 21:33:52       -0.262208        0.872117
BFGS:   87 21:33:53       -0.011586        0.539705
BFGS:   88 21:33:53        0.268809        0.556704
BFGS:   89 21:33:54        0.396439        0.930903
BFGS:   90 21:33:54        0.476540        1.183376
BFGS:   91 21:33:55        0.541018        1.238261
BFGS:   92 21:33:55        0.673771        1.320231
BFGS:   93 21:33:56        0.617221        1.355538
BFGS:   94 21:33:57        0.511605        1.250702
BFGS:   95 21:33:57        0.636878        1.609831
BFGS:   96 21:33:58        0.953523        2.103536
BFGS:   97 21:33:58        0.993933        3.057794
BFGS:   98 21:33:59        1.465081        3.702325
BFGS:   99 21:33:59        1.560205        3.903290
BFGS:  100 21:34:00        2.102821        3.686313
BFGS:  101 21:34:00        1.963662        3.551537
BFGS:  102 21:34:01        1.253160        3.428253
BFGS:  103 21:34:01        0.123423        2.898158
BFGS:  104 21:34:02       -0.375245        2.272291
BFGS:  105 21:34:02       -0.423563        2.254666
BFGS:  106 21:34:03       -0.542788        2.470937
BFGS:  107 21:34:04       -0.580909        1.194042
BFGS:  108 21:34:04       -0.559024        1.386672
BFGS:  109 21:34:05       -0.481440        1.867881
BFGS:  110 21:34:05       -0.120945        2.264150
BFGS:  111 21:34:06        0.444238        2.451535
BFGS:  112 21:34:06        0.731881        2.386690
BFGS:  113 21:34:07        0.751464        2.094409
BFGS:  114 21:34:07        0.603834        1.511794
BFGS:  115 21:34:08        0.542250        0.865894
BFGS:  116 21:34:09        0.532161        0.812582
BFGS:  117 21:34:10        0.402585        0.620228
BFGS:  118 21:34:11        0.385078        0.360302
BFGS:  119 21:34:13        0.417862        0.305388
BFGS:  120 21:34:16        0.470009        0.326081
BFGS:  121 21:34:18        0.565799        0.494168
BFGS:  122 21:34:19        0.684843        1.150160
BFGS:  123 21:34:19        0.907505        1.740264
BFGS:  124 21:34:20        0.988664        2.052202
BFGS:  125 21:34:21        0.965027        2.324502
BFGS:  126 21:34:21        0.328837        2.485502
BFGS:  127 21:34:21       -0.317497        2.331713
BFGS:  128 21:34:22       -0.430573        2.768586
BFGS:  129 21:34:22        0.106877        5.121327
BFGS:  130 21:34:23        0.139761        6.260367
BFGS:  131 21:34:23        0.847748        5.616812
BFGS:  132 21:34:24        2.059264        3.627952
BFGS:  133 21:34:24        4.654224        6.663556
BFGS:  134 21:34:25        7.161179       12.522941
BFGS:  135 21:34:25        6.068144       14.277150
BFGS:  136 21:34:25        2.170208       10.141011
BFGS:  137 21:34:26        2.028040        7.377157
BFGS:  138 21:34:26        1.730923        3.724474
BFGS:  139 21:34:27        1.183573        3.192833
BFGS:  140 21:34:27       -0.589020        2.185126
BFGS:  141 21:34:27       -0.602139        2.332437
BFGS:  142 21:34:28       -0.630542        2.070743
BFGS:  143 21:34:28       -0.655382        1.558621
BFGS:  144 21:34:29       -0.643458        1.570425
BFGS:  145 21:34:29       -0.406476        2.894044
BFGS:  146 21:34:30        0.804249       10.897558
BFGS:  147 21:34:30        3.400803       29.023109
BFGS:  148 21:34:31        8.029625       50.667454
BFGS:  149 21:34:31       12.394200       61.187607
BFGS:  150 21:34:31       12.939157       65.788923
BFGS:  151 21:34:32       18.721682       96.742717
BFGS:  152 21:34:32       18.836111       88.267356
BFGS:  153 21:34:33       18.611809       53.386788
BFGS:  154 21:34:33       18.500793       40.843761
BFGS:  155 21:34:34       15.953651       39.418034
BFGS:  156 21:34:35       13.597256       44.794808
BFGS:  157 21:34:35       15.308739       35.344536
BFGS:  158 21:34:36       16.735186       29.605895
